#!/usr/bin/env python3
"""
Script de Compara√ß√£o: Endpoint Antigo vs Novos Endpoints V2
Compara dados retornados para validar compatibilidade e corre√ß√£o
"""

import requests
import json
import time
from datetime import datetime
from typing import Dict, Any, List

# Configura√ß√£o
BASE_URL = "http://localhost:8000"

class EndpointComparator:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.results = {}
        
    def make_request(self, endpoint: str, params: Dict = None) -> Dict[str, Any]:
        """Faz requisi√ß√£o e mede tempo de resposta"""
        url = f"{self.base_url}{endpoint}"
        
        start_time = time.time()
        try:
            response = requests.get(url, params=params, timeout=30)
            end_time = time.time()
            
            response_time = (end_time - start_time) * 1000  # em ms
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "data": data,
                    "response_time": response_time,
                    "size_bytes": len(response.content),
                    "status_code": response.status_code
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time": response_time,
                    "size_bytes": len(response.content),
                    "status_code": response.status_code
                }
                
        except requests.exceptions.Timeout:
            return {"success": False, "error": "Timeout (30s)", "response_time": 30000}
        except requests.exceptions.ConnectionError:
            return {"success": False, "error": "Connection Error", "response_time": 0}
        except Exception as e:
            return {"success": False, "error": str(e), "response_time": 0}
    
    def test_old_endpoint(self, params: Dict = None) -> Dict[str, Any]:
        """Testa o endpoint antigo pesado"""
        print("üîç Testando endpoint ANTIGO: /dashboard/sales-complete")
        
        result = self.make_request("/dashboard/sales-complete", params)
        
        if result["success"]:
            data = result["data"]
            print(f"   ‚úÖ Sucesso - {result['response_time']:.0f}ms - {result['size_bytes']} bytes")
            
            # Extrair m√©tricas principais
            metrics = {
                "totalLeads": data.get("totalLeads", 0),
                "leadsByUser": len(data.get("leadsByUser", [])),
                "conversionRates": data.get("conversionRates", {}),
                "leadsByStage": len(data.get("leadsByStage", [])),
                "performance": {
                    "response_time": result["response_time"],
                    "size_bytes": result["size_bytes"]
                }
            }
            
            return {"success": True, "metrics": metrics, "raw_data": data}
        else:
            print(f"   ‚ùå Erro: {result.get('error', 'Unknown')}")
            return {"success": False, "error": result.get("error")}
    
    def test_new_endpoints(self, params: Dict = None) -> Dict[str, Any]:
        """Testa os novos endpoints V2"""
        print("üöÄ Testando endpoints NOVOS V2:")
        
        endpoints = [
            ("/api/v2/sales/kpis", "KPIs"),
            ("/api/v2/charts/leads-by-user", "Leads por Usu√°rio"),
            ("/api/v2/sales/conversion-rates", "Taxas de Convers√£o"),
            ("/api/v2/sales/pipeline-status", "Status Pipeline")
        ]
        
        total_time = 0
        total_size = 0
        aggregated_data = {}
        
        for endpoint, name in endpoints:
            print(f"   üîç {name}...")
            result = self.make_request(endpoint, params)
            
            if result["success"]:
                total_time += result["response_time"]
                total_size += result["size_bytes"]
                aggregated_data[name] = result["data"]
                print(f"      ‚úÖ {result['response_time']:.0f}ms - {result['size_bytes']} bytes")
            else:
                print(f"      ‚ùå Erro: {result.get('error', 'Unknown')}")
                return {"success": False, "error": f"Falha em {name}: {result.get('error')}"}
        
        # Agregar m√©tricas dos novos endpoints
        kpis = aggregated_data.get("KPIs", {})
        leads_by_user = aggregated_data.get("Leads por Usu√°rio", {})
        conversion_rates = aggregated_data.get("Taxas de Convers√£o", {})
        pipeline_status = aggregated_data.get("Status Pipeline", {})
        
        metrics = {
            "totalLeads": kpis.get("totalLeads", 0),
            "leadsByUser": len(leads_by_user.get("leadsByUser", [])),
            "conversionRates": conversion_rates.get("conversionRates", {}),
            "leadsByStage": len(pipeline_status.get("pipelineStatus", [])),
            "performance": {
                "response_time": total_time,
                "size_bytes": total_size
            }
        }
        
        return {
            "success": True, 
            "metrics": metrics, 
            "raw_data": aggregated_data,
            "individual_times": {name: result["response_time"] for (_, name), result in zip(endpoints, [self.make_request(ep, params) for ep, _ in endpoints])}
        }
    
    def compare_metrics(self, old_metrics: Dict, new_metrics: Dict) -> Dict[str, Any]:
        """Compara m√©tricas entre endpoints antigo e novo"""
        comparison = {
            "data_accuracy": {},
            "performance": {},
            "compatibility": True
        }
        
        # Comparar dados
        for metric in ["totalLeads", "leadsByUser", "conversionRates"]:
            old_val = old_metrics.get(metric)
            new_val = new_metrics.get(metric)
            
            if isinstance(old_val, (int, float)) and isinstance(new_val, (int, float)):
                # Comparar valores num√©ricos
                diff = abs(old_val - new_val)
                diff_percent = (diff / max(old_val, 1)) * 100
                
                comparison["data_accuracy"][metric] = {
                    "old_value": old_val,
                    "new_value": new_val,
                    "difference": diff,
                    "difference_percent": round(diff_percent, 2),
                    "match": diff_percent < 5  # 5% de toler√¢ncia
                }
            elif isinstance(old_val, dict) and isinstance(new_val, dict):
                # Comparar objetos (como conversionRates)
                matches = 0
                total = len(old_val)
                
                for key in old_val:
                    if key in new_val:
                        old_rate = old_val[key]
                        new_rate = new_val[key]
                        if isinstance(old_rate, (int, float)) and isinstance(new_rate, (int, float)):
                            diff_percent = abs(old_rate - new_rate) / max(old_rate, 1) * 100
                            if diff_percent < 10:  # 10% toler√¢ncia para rates
                                matches += 1
                
                comparison["data_accuracy"][metric] = {
                    "old_value": old_val,
                    "new_value": new_val,
                    "matching_keys": matches,
                    "total_keys": total,
                    "match_rate": (matches / max(total, 1)) * 100
                }
            else:
                comparison["data_accuracy"][metric] = {
                    "old_value": old_val,
                    "new_value": new_val,
                    "match": old_val == new_val
                }
        
        # Comparar performance
        old_perf = old_metrics.get("performance", {})
        new_perf = new_metrics.get("performance", {})
        
        comparison["performance"] = {
            "response_time": {
                "old_ms": old_perf.get("response_time", 0),
                "new_ms": new_perf.get("response_time", 0),
                "improvement_factor": old_perf.get("response_time", 1) / max(new_perf.get("response_time", 1), 1),
                "improvement_percent": ((old_perf.get("response_time", 0) - new_perf.get("response_time", 0)) / max(old_perf.get("response_time", 1), 1)) * 100
            },
            "payload_size": {
                "old_bytes": old_perf.get("size_bytes", 0),
                "new_bytes": new_perf.get("size_bytes", 0),
                "reduction_factor": old_perf.get("size_bytes", 1) / max(new_perf.get("size_bytes", 1), 1),
                "reduction_percent": ((old_perf.get("size_bytes", 0) - new_perf.get("size_bytes", 0)) / max(old_perf.get("size_bytes", 1), 1)) * 100
            }
        }
        
        return comparison
    
    def run_comparison(self, test_params: List[Dict] = None):
        """Executa compara√ß√£o completa"""
        if test_params is None:
            test_params = [
                {"days": 30},  # Teste padr√£o
                {"days": 7},   # Teste per√≠odo menor
                {"days": 30, "corretor": "Alexandre Perazzo"},  # Teste com filtro
            ]
        
        print("=" * 80)
        print("üî• COMPARA√á√ÉO: ENDPOINT ANTIGO vs ENDPOINTS V2")
        print("=" * 80)
        print(f"üïê Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üåç Base URL: {self.base_url}")
        
        all_results = []
        
        for i, params in enumerate(test_params, 1):
            print(f"\nüìä TESTE {i}: {params}")
            print("-" * 50)
            
            # Testar endpoint antigo
            old_result = self.test_old_endpoint(params)
            
            print()
            
            # Testar novos endpoints
            new_result = self.test_new_endpoints(params)
            
            if old_result["success"] and new_result["success"]:
                # Comparar resultados
                comparison = self.compare_metrics(old_result["metrics"], new_result["metrics"])
                
                all_results.append({
                    "test_params": params,
                    "old_result": old_result,
                    "new_result": new_result,
                    "comparison": comparison
                })
                
                print(f"\nüìà COMPARA√á√ÉO:")
                print(f"   Total Leads - Antigo: {old_result['metrics']['totalLeads']}, Novo: {new_result['metrics']['totalLeads']}")
                print(f"   Usu√°rios - Antigo: {old_result['metrics']['leadsByUser']}, Novo: {new_result['metrics']['leadsByUser']}")
                print(f"   Performance - Antigo: {old_result['metrics']['performance']['response_time']:.0f}ms, Novo: {new_result['metrics']['performance']['response_time']:.0f}ms")
                
                improvement = comparison["performance"]["response_time"]["improvement_factor"]
                print(f"   üöÄ Melhoria: {improvement:.1f}x mais r√°pido")
                
            else:
                print("‚ùå N√£o foi poss√≠vel comparar devido a erros")
        
        # Resumo final
        self.print_final_summary(all_results)
        
        return all_results
    
    def print_final_summary(self, results: List[Dict]):
        """Imprime resumo final da compara√ß√£o"""
        if not results:
            print("\n‚ùå Nenhum teste foi bem-sucedido")
            return
        
        print("\n" + "=" * 80)
        print("üìã RESUMO FINAL")
        print("=" * 80)
        
        # Calcular m√©dias
        total_tests = len(results)
        successful_tests = len([r for r in results if r["comparison"]])
        
        avg_old_time = sum(r["old_result"]["metrics"]["performance"]["response_time"] for r in results) / total_tests
        avg_new_time = sum(r["new_result"]["metrics"]["performance"]["response_time"] for r in results) / total_tests
        
        avg_old_size = sum(r["old_result"]["metrics"]["performance"]["size_bytes"] for r in results) / total_tests
        avg_new_size = sum(r["new_result"]["metrics"]["performance"]["size_bytes"] for r in results) / total_tests
        
        print(f"‚úÖ Testes bem-sucedidos: {successful_tests}/{total_tests}")
        print(f"\n‚ö° PERFORMANCE:")
        print(f"   Tempo m√©dio - Antigo: {avg_old_time:.0f}ms | Novo: {avg_new_time:.0f}ms")
        print(f"   Melhoria de tempo: {avg_old_time/avg_new_time:.1f}x mais r√°pido")
        print(f"   Tamanho m√©dio - Antigo: {avg_old_size/1024:.1f}KB | Novo: {avg_new_size/1024:.1f}KB")
        print(f"   Redu√ß√£o de tamanho: {avg_old_size/avg_new_size:.1f}x menor")
        
        # Verificar consist√™ncia dos dados
        data_issues = []
        for i, result in enumerate(results, 1):
            comp = result["comparison"]["data_accuracy"]
            for metric, data in comp.items():
                if isinstance(data, dict) and not data.get("match", True):
                    if isinstance(data.get("difference_percent"), (int, float)) and data["difference_percent"] > 5:
                        data_issues.append(f"Teste {i} - {metric}: {data['difference_percent']:.1f}% diferen√ßa")
        
        if data_issues:
            print(f"\n‚ö†Ô∏è  PROBLEMAS DE CONSIST√äNCIA:")
            for issue in data_issues:
                print(f"   - {issue}")
        else:
            print(f"\n‚úÖ CONSIST√äNCIA DE DADOS: Todos os dados est√£o consistentes!")
        
        print(f"\nüéØ CONCLUS√ÉO:")
        if successful_tests == total_tests and not data_issues:
            print("   ‚úÖ ENDPOINTS V2 EST√ÉO CORRETOS E FUNCIONAIS!")
            print("   üöÄ Performance significativamente melhorada")
            print("   üìä Dados consistentes com endpoint original")
            print("   üéâ FRONTEND PODE MIGRAR COM SEGURAN√áA!")
        else:
            print("   ‚ö†Ô∏è  Alguns problemas encontrados - revisar implementa√ß√£o")
        
        print("=" * 80)

def main():
    """Fun√ß√£o principal"""
    comparator = EndpointComparator(BASE_URL)
    
    # Verificar conectividade
    print("üîå Verificando conectividade...")
    try:
        response = requests.get(f"{BASE_URL}/", timeout=5)
        print("‚úÖ Servidor est√° rodando")
    except:
        print("‚ùå Servidor n√£o est√° acess√≠vel")
        print("üí° Certifique-se de que o servidor est√° rodando com: python main.py")
        return
    
    # Configurar testes
    test_scenarios = [
        {"days": 30},  # Teste b√°sico
        {"days": 7},   # Per√≠odo menor
        {"days": 30, "fonte": "Google"},  # Teste com filtro de fonte
        {"days": 30, "corretor": "Alexandre Perazzo"},  # Teste com filtro de corretor
    ]
    
    # Executar compara√ß√£o
    results = comparator.run_comparison(test_scenarios)
    
    # Salvar resultados detalhados
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"comparison_results_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nüìÑ Resultados detalhados salvos em: {filename}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  N√£o foi poss√≠vel salvar resultados: {e}")

if __name__ == "__main__":
    main()